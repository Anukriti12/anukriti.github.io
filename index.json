[{"authors":null,"categories":null,"content":"Hi! I am a Research Fellow at Microsoft Research, where I am working with Dr. Amit Sharma and Dr. Emre Kiciman. My research focuses on causal representation learning and I am broadly interested in developing robust and generalizable systems possessing grounded language understanding.\nPrior to this, I was a research intern at Adobe\u0026rsquo;s Media and Data Science Research (MDSR) lab, where I worked on knowledge enhancement of language models to make robust factual predictions. I also contributed to a project on task-agnostic generalizable commonsense reasoning.\nI graduated from BITS Pilani, Pilani with a Bachelor\u0026rsquo;s in Computer Science in 2021. I was fortunate to pursue my undergraduate thesis at the MultiComp Lab in the Language Technologies Institute of Carnegie Mellon University, supervised by Prof. Louis-Philippe Morency and mentored by Paul Pu Liang and Yiding Jiang. My thesis was focused on multimodal reinforcement learning, specifically exploring how language grounding can assist in capturing affordance of objects and accelerate learning of autonomous agents.\nIn the past, I collaborated with the Language Technology Lab, Universit√§t Hamburg, where I felt grateful to be working under the supervision of Prof. Dr. Chris Biemann on query building and question answering over knowledge graphs. Previously, I have interned as a software engineer at Microsoft, Bangalore and worked on computer vision problems at MapmyIndia, Delhi.\nPersonal: My childhood fascination with space (still excites me) led me to joining Team Anant, a group of passionate undergraduate students building BITS Pilani\u0026rsquo;s first nanosatellite, where I was fortunate to work with some amazing people. I love running, playing basketball and am trying to find time to revisit my passion to write.\n","date":1631380402,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631380402,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jivatneet.github.io/author/jivat-neet-kaur/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jivat-neet-kaur/","section":"authors","summary":"Hi! I am a Research Fellow at Microsoft Research, where I am working with Dr. Amit Sharma and Dr. Emre Kiciman. My research focuses on causal representation learning and I am broadly interested in developing robust and generalizable systems possessing grounded language understanding.","tags":null,"title":"Jivat Neet Kaur","type":"authors"},{"authors":["Jivat Neet Kaur","Sumit Bhatia","Milan Aggarwal","Rachit Bansal","Balaji Krishnamurthy"],"categories":[],"content":"","date":1631380402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631380402,"objectID":"5a0eae2c8a13a57556c0a47f4841697f","permalink":"https://jivatneet.github.io/publication/knowledgebert/","publishdate":"2021-03-26T22:43:22+05:30","relpermalink":"/publication/knowledgebert/","section":"publication","summary":"Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture semantic, syntactic, and factual knowledge in their parameters. However, storing large amounts of factual knowledge in the parameters of the model is sub-optimal given the resource requirements and ever-growing amounts of knowledge. Instead of packing all the knowledge in the model parameters, we argue that a more efficient alternative is to provide contextually relevant structured knowledge to the model and train it to use that knowledge. This allows the training of the language model to be de-coupled from the external knowledge source and the latter can be updated without affecting the parameters of the language model. Empirical evaluation using different subsets of LAMA probe reveals that such an approach allows smaller language models with access to external knowledge to achieve significant and robust outperformance over much larger language models.","tags":[],"title":"No Need to Know Everything! Efficiently Augmenting Language Models With External Knowledge","type":"publication"},{"authors":["Rachit Bansal","Milan Aggarwal","Sumit Bhatia","Jivat Neet Kaur","Balaji Krishnamurthy"],"categories":[],"content":"","date":1631294002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631294002,"objectID":"745a913e357b1e82897685898fb79917","permalink":"https://jivatneet.github.io/publication/coseco/","publishdate":"2021-03-26T22:43:22+05:30","relpermalink":"/publication/coseco/","section":"publication","summary":"Pre-trained Language Models (PTLMs) have been shown to perform well on natural language reasoning tasks requiring commonsense. Prior work has leveraged structured commonsense present in knowledge graphs (KGs) to assist PTLMs. Some of these methods use KGs as separate static modules which limits knowledge coverage since KGs are finite, sparse, and noisy. Other methods have attempted to obtain generalized and scalable commonsense by training PTLMs on KGs. Since they are trained on symbolic KG phrases, applying them on natural language text during inference leads to input distribution shift. To this end, we propose a task agnostic sentence-conditioned generative CommonSense Contextualizer (CoSe-Co), which is trained to generate contextually relevant commonsense inferences given a natural language input. We devise a method to create semantically related sentence-commonsense pairs to train CoSe-Co. We observe commonsense inferences generated by CoSe-Co contain novel concepts that are relevant to the entire sentence context. We evaluate CoSe-Co on multi-choice QA and open-ended commonsense reasoning tasks on the CSQA, ARC, QASC, and OBQA datasets. CoSe-Co outperforms state-of-the-art methods in both these settings, while being task-agnostic, and performs especially well in low data regimes showing it is more robust and generalises better.","tags":[],"title":"CoSe-Co: Sentence Conditioned Generative CommonSense Contextualizer for Language Models","type":"publication"},{"authors":[],"categories":[],"content":"Built a multi-scale deep CNN architecture using Keras to learn Deep Motor Features for brain computer interface with imagined motor tasks in BCI - EEG motor activity dataset.\n","date":1607611783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607611783,"objectID":"a4c7bf8db2fe884432e6a69fd64be782","permalink":"https://jivatneet.github.io/project/bci/","publishdate":"2020-12-10T20:19:43+05:30","relpermalink":"/project/bci/","section":"project","summary":"Built a multi-scale deep CNN architecture using Keras to learn Deep Motor Features for brain computer interface with imagined motor tasks in BCI - EEG motor activity dataset.","tags":[],"title":"Deep Learning EEG Response Representation for BCI","type":"project"},{"authors":[],"categories":[],"content":"","date":1607543802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607543802,"objectID":"438bcd32f31c5cf276057cf5403b8d53","permalink":"https://jivatneet.github.io/project/socketprog/","publishdate":"2020-12-10T01:26:42+05:30","relpermalink":"/project/socketprog/","section":"project","summary":"Implementation of File transfer using multi-channel stop-and-wait (TCP) and Selective Repeat (UDP) protocols","tags":[],"title":"Socket Programming","type":"project"},{"authors":[],"categories":[],"content":"","date":1607540607,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607540607,"objectID":"c6936d3873c5d574db9dc5e0a11e412f","permalink":"https://jivatneet.github.io/project/covinfo/","publishdate":"2020-12-10T00:33:27+05:30","relpermalink":"/project/covinfo/","section":"project","summary":"Developed a web application for real-time hospital resource monitoring (beds, ICUs, ventilators). Integrated a mask detection model to provide real-time information regarding the percentage of people wearing masks at any location using live video feed","tags":[],"title":"COVINFO Application","type":"project"},{"authors":[],"categories":[],"content":"","date":1607540081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607540081,"objectID":"cbbeb1b003fbde7fb974dc4f6e38f4ef","permalink":"https://jivatneet.github.io/project/ner/","publishdate":"2020-12-10T00:24:41+05:30","relpermalink":"/project/ner/","section":"project","summary":"Trained word embeddings using Word2Vec's Skip-Gram model and created a window-based classification Network in Python for learning Named Entity classes.","tags":[],"title":"Named entity recognition (NER)","type":"project"},{"authors":[],"categories":[],"content":"","date":1607538923,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607538923,"objectID":"b0fad77f507e2f8538af926b6ba0f335","permalink":"https://jivatneet.github.io/project/compiler/","publishdate":"2020-12-10T00:05:23+05:30","relpermalink":"/project/compiler/","section":"project","summary":"Developed a fully functional compiler from scratch (in C) capable of lexical analysis, syntax tree creation, semantic analysis, static and dynamic type checking and generating executable assembly code. The artificial language supported constructs like dynamic memory allocation, loops, if-else ladders, switch statements, nested scopes and function calls","tags":[],"title":"Compiler Design for a Custom Language","type":"project"},{"authors":["Jivat Neet Kaur","Yiding Jiang","Paul Pu Liang"],"categories":[],"content":"","date":1571678002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571678002,"objectID":"f4834bee4a188735ad03c9844a512ca6","permalink":"https://jivatneet.github.io/publication/iclr-eml/","publishdate":"2021-03-26T22:43:22+05:30","relpermalink":"/publication/iclr-eml/","section":"publication","summary":"In many real-world scenarios where extrinsic rewards to the agent are extremely sparse, curiosity has emerged as a useful concept providing intrinsic rewards that enable the agent to explore its environment and acquire knowledge to achieve its goals. Despite their strong performance on many sparse-reward tasks, existing curiosity approaches rely on a holistic view of state transitions and do not allow for a structured understanding of specific aspects of the environment. In this paper, we formulate curiosity based on grounded question answering by encouraging the agent to ask questions about the environment and be curious when the answers to their questions change. We show that language questions encourage the agent to uncover specific knowledge about their environment such as the physical properties of objects as well as their spatial relationships with other objects, which serve as valuable curiosity rewards to solve sparse-reward tasks more efficiently.","tags":[],"title":"Ask \u0026 Explore: Grounded Question Answering for Curiosity-driven exploration","type":"publication"},{"authors":["Vishnu P Katkoori","Jivat Neet Kaur","Tushar Goyal"],"categories":[],"content":"","date":1571678002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571678002,"objectID":"ce6c6284310aae1e166ed8e168048f8c","permalink":"https://jivatneet.github.io/publication/bdot/","publishdate":"2020-12-06T22:43:22+05:30","relpermalink":"/publication/bdot/","section":"publication","summary":"As a satellite is deployed from the launch vehicle, it is subjected to high angular rates which need to be dampened in order for the satellite to perform its functions as expected. Simple and robust algorithms, such as BDot, are generally used to provide the required control torque for detumbling the satellite. This paper elucidates the design process for the detumbling algorithm to be implemented on a nanosatellite currently being developed by Team Anant, the Student Satellite Team of BITS Pilani. The process commenced with the selection of hardware to be used onboard the satellite. Magnetometers and Gyroscopes were finalized to be used as sensors. Various commercially available sensor models were then compared based on power and operating conditions. For actuation, a magnetorquer system was designed specifically to the requirements of the team. The system comprised of two magnetorquer rods and a magnetorquer coil aligned in orthogonal directions. The sensors and actuators were then accurately modeled in MATLAB for further testing. The modeling involved some interesting challenges due to the magnetic moment retained by the ferromagnetic core. These challenges, and the ways to overcome them have been also been briefly discussed in the paper. After finalizing the hardware, the team proceeded with implementing various popular control algorithms for detumbling the satellite. The algorithms were first theoretically analysed, and then modeled on MATLAB. The simulations took the space environment around the satellite into consideration for higher accuracy. The algorithms were tested for different initial conditions, using different time-steps and under different power constraints. The algorithms considered and the conclusions derived from these simulations have also been discussed elaborately in this paper. The paper concluded by presenting the finalized detumbling algorithm(s) to be used by Team Anant, and the various conditions devised to ensure efficient use of electrical power. The paper also presents viable alternatives to the finalized algorithm(s), using other hardware components. These alternatives and conditions have also been documented in the paper for a better understanding.","tags":[],"title":"Simulation and Selection of Detumbling Algorithms for a 3U CubeSat","type":"publication"}]